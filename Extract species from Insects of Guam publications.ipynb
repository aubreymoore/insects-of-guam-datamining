{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Run with Python 3 kernel."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import docker\n",
    "\n",
    "client = docker.from_env()\n",
    "\n",
    "for container in client.containers.list():\n",
    "    container.stop()\n",
    "\n",
    "data = container = client.containers.run('cvat', detach=True)\n",
    "print(data)\n",
    "file_json = data.get_archive('testdump.xml')\n",
    "stream, stat = file_json\n",
    "file_obj = BytesIO()\n",
    "for i in stream:\n",
    "    file_obj.write(i)\n",
    "file_obj.seek(0)\n",
    "tar = tarfile.open(mode='r', fileobj=file_obj)\n",
    "text = tar.extractfile('out.json')\n",
    "q = text.read()\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bishop_bulletins_page(signature):\n",
    "    \"\"\"\n",
    "    Scrapes data from the Bishop Museum pubs online web page. \n",
    "    Signature is b172 for Insects of Guam I and b189 for Insects of Guam II.\n",
    "    A directory named b172 or b189 is created and populated with a CSV file, named b189.csv, containing\n",
    "    titles, stubs, authors, and urls for pdfs.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin\n",
    "    from slugify import slugify\n",
    "    \n",
    "    if signature not in ['b172', 'b189']:\n",
    "        print(\"Signature not in ['b172', 'b189']\")\n",
    "        return\n",
    "    \n",
    "    if os.path.exists(signature):\n",
    "        print('{} directory already exists.'.format(signature))\n",
    "        return   \n",
    "\n",
    "    bulletins_url = 'http://hbs.bishopmuseum.org/pubs-online/bpbm-bulletins.html'\n",
    "    result = requests.get(bulletins_url)\n",
    "    soup = BeautifulSoup(result.content, features = \"lxml\")\n",
    "\n",
    "    pdf_list = []\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        if signature in link.get('href'):\n",
    "            text = link.previous_sibling.previous_sibling.previous_sibling\n",
    "            parts = text.split(', by ')\n",
    "            if len(parts) == 2:\n",
    "                title = parts[0].strip()\n",
    "                slug = slugify(title)\n",
    "                authors = parts[1].replace('[','').strip()          \n",
    "                url = link.get('href')\n",
    "                url = urljoin(bulletins_url, url)\n",
    "                pdf_list.append({'title':title, 'slug':slug, 'authors':authors, 'url':url})\n",
    "    \n",
    "    df_pdf_list = pd.DataFrame(pdf_list)\n",
    "    os.mkdir(signature)\n",
    "    outfile = '{}/{}.csv'.format(signature, signature)\n",
    "    df_pdf_list.to_csv(outfile, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "## Usage example:\n",
    "#scrape_bishop_bulletins_page('b172')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aubrey/insects-of-guam-test/b172\n",
      "dragonflies-of-guam directory already exists.\n",
      "thrips-of-guam directory already exists.\n",
      "cercopidae-of-guam directory already exists.\n",
      "membracidae-of-guam directory already exists.\n",
      "psyllidae-from-guam directory already exists.\n",
      "aphidae-and-aleurodidae-of-guam directory already exists.\n",
      "neuropteroid-insects-from-guam directory already exists.\n",
      "butterflies-of-guam directory already exists.\n",
      "sphingidae-of-guam directory already exists.\n",
      "staphylinidae-of-guam directory already exists.\n",
      "rhipiceridae-of-guam directory already exists.\n",
      "ciidae-of-guam directory already exists.\n",
      "elaterid-and-eucnemid-beetles-of-guam directory already exists.\n",
      "coleoptera-heteromera-from-guam directory already exists.\n",
      "new-longicorn-beetles-from-guam-cerambycidae directory already exists.\n",
      "anthribidae-of-guam directory already exists.\n",
      "curculionidae-of-guam directory already exists.\n",
      "barkbeetles-of-guam directory already exists.\n",
      "miscellaneous-families-of-guam-coleoptera directory already exists.\n",
      "stylopidae-of-guam directory already exists.\n",
      "formicidae-of-guam directory already exists.\n",
      "wasps-of-guam directory already exists.\n",
      "bees-of-guam directory already exists.\n",
      "halictine-bees-from-rota-island directory already exists.\n",
      "tipulidae-of-guam directory already exists.\n",
      "culicidae-of-guam directory already exists.\n",
      "trypetidae-otitidae-helomyzidae-and-clusiidae-of-guam-diptera directory already exists.\n",
      "/home/aubrey/insects-of-guam-test\n"
     ]
    }
   ],
   "source": [
    "def create_file_structure(signature):\n",
    "    \"\"\"\n",
    "    Signature is 'b172' for Insects of Guam I and 'b189' for Insects of Guam II.\n",
    "    Depends on scrape_bishop_bulletins_page\n",
    "    Creates a data file structure in this format:\n",
    "    \n",
    "    b172\n",
    "        anthribidae-of-guam\n",
    "            anthribidae-of-guam.pdf\n",
    "            anthribidae-of-guam-0.jpg\n",
    "            anthribidae-of-guam-1.jpg\n",
    "            ...\n",
    "        formicidae-of-guam\n",
    "            formicidae-of-guam.pdf\n",
    "            formicidae-of-guam-0.jpg\n",
    "            formicidae-of-guam-1.jpg\n",
    "            ...\n",
    "            \n",
    "    Each directory contains a PDF file and a JPG image for each page in the PDF.\n",
    "    \n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import os\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    \n",
    "    if signature not in ['b172', 'b189']:\n",
    "        print(\"Signature not in ['b172', 'b189']\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(signature):\n",
    "        print('{} directory does not exist.'.format(signature))\n",
    "        print('Scraping Bishop Museum Bulletins web page.')\n",
    "        scrape_bishop_bulletins_page(signature)\n",
    "        return\n",
    "    \n",
    "    pdf_list = pd.read_csv('{}/{}.csv'.format(signature, signature)).to_dict('records')\n",
    "    \n",
    "    os.chdir('b172'); print(os.getcwd())\n",
    "    for d in pdf_list:\n",
    "        slug = d['slug']\n",
    "        if os.path.exists(slug):\n",
    "            print('{} directory already exists.'.format(slug))\n",
    "            continue\n",
    "\n",
    "        # Create a new directory and move into it\n",
    "        url = d['url']\n",
    "        os.mkdir(slug); os.chdir(slug); print(os.getcwd())\n",
    "\n",
    "        # Download PDF\n",
    "        filename = '{}.pdf'.format(slug)\n",
    "        r = requests.get(url)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "\n",
    "        # Create a JPG image for each page in PDF using the Linux convert command\n",
    "        jpg = filename.replace('pdf', 'jpg')\n",
    "        subprocess.call(['convert', '-density', '200x200', filename, jpg])\n",
    "\n",
    "        # Move up one directory\n",
    "        os.chdir('..'); print(os.getcwd())\n",
    "    os.chdir('..'); print(os.getcwd())\n",
    "    return\n",
    "\n",
    "create_file_structure('b172')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bb_image</th>\n",
       "      <th>page_image</th>\n",
       "      <th>species_name</th>\n",
       "      <th>xbr</th>\n",
       "      <th>xtl</th>\n",
       "      <th>ybr</th>\n",
       "      <th>ytl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jordanthribus-planifascietus-0.jpg</td>\n",
       "      <td>anthribidae-of-guam-1.jpg</td>\n",
       "      <td>Jordanthribus planifascietus</td>\n",
       "      <td>1243</td>\n",
       "      <td>282</td>\n",
       "      <td>1718</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jordanthribus-planifascietus-1.jpg</td>\n",
       "      <td>anthribidae-of-guam-2.jpg</td>\n",
       "      <td>Jordanthribus planifascietus</td>\n",
       "      <td>1162</td>\n",
       "      <td>191</td>\n",
       "      <td>932</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jordanthribus-conspersus-2.jpg</td>\n",
       "      <td>anthribidae-of-guam-2.jpg</td>\n",
       "      <td>Jordanthribus conspersus</td>\n",
       "      <td>1157</td>\n",
       "      <td>186</td>\n",
       "      <td>1692</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jordanthribus-conspersus-3.jpg</td>\n",
       "      <td>anthribidae-of-guam-3.jpg</td>\n",
       "      <td>Jordanthribus conspersus</td>\n",
       "      <td>1245</td>\n",
       "      <td>271</td>\n",
       "      <td>1009</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Notioxenus-fulgidus-4.jpg</td>\n",
       "      <td>anthribidae-of-guam-3.jpg</td>\n",
       "      <td>Notioxenus fulgidus</td>\n",
       "      <td>1250</td>\n",
       "      <td>273</td>\n",
       "      <td>1727</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Notioxenus-fulgidus-5.jpg</td>\n",
       "      <td>anthribidae-of-guam-4.jpg</td>\n",
       "      <td>Notioxenus fulgidus</td>\n",
       "      <td>1167</td>\n",
       "      <td>188</td>\n",
       "      <td>1474</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Melanopsacus-parvulus-6.jpg</td>\n",
       "      <td>anthribidae-of-guam-4.jpg</td>\n",
       "      <td>Melanopsacus parvulus</td>\n",
       "      <td>1167</td>\n",
       "      <td>190</td>\n",
       "      <td>1681</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Melanopsacus-parvulus-7.jpg</td>\n",
       "      <td>anthribidae-of-guam-5.jpg</td>\n",
       "      <td>Melanopsacus parvulus</td>\n",
       "      <td>1236</td>\n",
       "      <td>267</td>\n",
       "      <td>1266</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mauia-subnotatus-8.jpg</td>\n",
       "      <td>anthribidae-of-guam-5.jpg</td>\n",
       "      <td>Mauia subnotatus</td>\n",
       "      <td>1236</td>\n",
       "      <td>275</td>\n",
       "      <td>1721</td>\n",
       "      <td>1459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mauia-subnotatus-9.jpg</td>\n",
       "      <td>anthribidae-of-guam-6.jpg</td>\n",
       "      <td>Mauia subnotatus</td>\n",
       "      <td>1148</td>\n",
       "      <td>181</td>\n",
       "      <td>604</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Araecerus-fasciculatus-10.jpg</td>\n",
       "      <td>anthribidae-of-guam-6.jpg</td>\n",
       "      <td>Araecerus fasciculatus</td>\n",
       "      <td>1146</td>\n",
       "      <td>176</td>\n",
       "      <td>1673</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Araecerus-vieillardi-11.jpg</td>\n",
       "      <td>anthribidae-of-guam-7.jpg</td>\n",
       "      <td>Araecerus vieillardi</td>\n",
       "      <td>1254</td>\n",
       "      <td>276</td>\n",
       "      <td>848</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Araeocorynus-cumingi-12.jpg</td>\n",
       "      <td>anthribidae-of-guam-7.jpg</td>\n",
       "      <td>Araeocorynus cumingi</td>\n",
       "      <td>1247</td>\n",
       "      <td>290</td>\n",
       "      <td>1391</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              bb_image                 page_image  \\\n",
       "0   Jordanthribus-planifascietus-0.jpg  anthribidae-of-guam-1.jpg   \n",
       "1   Jordanthribus-planifascietus-1.jpg  anthribidae-of-guam-2.jpg   \n",
       "2       Jordanthribus-conspersus-2.jpg  anthribidae-of-guam-2.jpg   \n",
       "3       Jordanthribus-conspersus-3.jpg  anthribidae-of-guam-3.jpg   \n",
       "4            Notioxenus-fulgidus-4.jpg  anthribidae-of-guam-3.jpg   \n",
       "5            Notioxenus-fulgidus-5.jpg  anthribidae-of-guam-4.jpg   \n",
       "6          Melanopsacus-parvulus-6.jpg  anthribidae-of-guam-4.jpg   \n",
       "7          Melanopsacus-parvulus-7.jpg  anthribidae-of-guam-5.jpg   \n",
       "8               Mauia-subnotatus-8.jpg  anthribidae-of-guam-5.jpg   \n",
       "9               Mauia-subnotatus-9.jpg  anthribidae-of-guam-6.jpg   \n",
       "10       Araecerus-fasciculatus-10.jpg  anthribidae-of-guam-6.jpg   \n",
       "11         Araecerus-vieillardi-11.jpg  anthribidae-of-guam-7.jpg   \n",
       "12         Araeocorynus-cumingi-12.jpg  anthribidae-of-guam-7.jpg   \n",
       "\n",
       "                    species_name   xbr  xtl   ybr   ytl  \n",
       "0   Jordanthribus planifascietus  1243  282  1718  1387  \n",
       "1   Jordanthribus planifascietus  1162  191   932   223  \n",
       "2       Jordanthribus conspersus  1157  186  1692   947  \n",
       "3       Jordanthribus conspersus  1245  271  1009   256  \n",
       "4            Notioxenus fulgidus  1250  273  1727  1251  \n",
       "5            Notioxenus fulgidus  1167  188  1474   221  \n",
       "6          Melanopsacus parvulus  1167  190  1681  1489  \n",
       "7          Melanopsacus parvulus  1236  267  1266   259  \n",
       "8               Mauia subnotatus  1236  275  1721  1459  \n",
       "9               Mauia subnotatus  1148  181   604   218  \n",
       "10        Araecerus fasciculatus  1146  176  1673  1205  \n",
       "11          Araecerus vieillardi  1254  276   848   246  \n",
       "12          Araeocorynus cumingi  1247  290  1391  1104  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def make_bounding_box_table(signature, section):\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "bulletin = 'b172'\n",
    "section = 'anthribidae-of-guam'\n",
    "\n",
    "f = open('b172/anthribidae-of-guam/anthribidae-of-guam.xml', 'r')\n",
    "contents = f.read()\n",
    "soup = BeautifulSoup(contents, features = \"lxml\")\n",
    "\n",
    "bb_list = []\n",
    "n = -1\n",
    "for image in soup.find_all('image'):\n",
    "    for box in image.find_all('box'):\n",
    "        n += 1\n",
    "        species_name = box.text.replace('\\n','')\n",
    "        bb_list.append({\n",
    "            'species_name':species_name, \n",
    "            'page_image':image['name'],\n",
    "            'bb_image': '{}-{}.jpg'.format(species_name.replace(' ', '-'), n),\n",
    "            'xtl':int(float(box['xtl'])),\n",
    "            'ytl':int(float(box['ytl'])),\n",
    "            'xbr':int(float(box['xbr'])),\n",
    "            'ybr':int(float(box['ybr']))\n",
    "        })\n",
    "df_bb_list = pd.DataFrame(bb_list)\n",
    "outfile = '{}/{}/bounding_boxes.csv'.format(bulletin, section)\n",
    "df_bb_list.to_csv(outfile, index=False)\n",
    "\n",
    "df_bb_list\n",
    "\n",
    "#make_bounding_box_table('b172', 'anthribidae-of-guam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jordanthribus-planifascietus-0.jpg\n",
      "Jordanthribus-planifascietus-1.jpg\n",
      "Jordanthribus-conspersus-2.jpg\n",
      "Jordanthribus-conspersus-3.jpg\n",
      "Notioxenus-fulgidus-4.jpg\n",
      "Notioxenus-fulgidus-5.jpg\n",
      "Melanopsacus-parvulus-6.jpg\n",
      "Melanopsacus-parvulus-7.jpg\n",
      "Mauia-subnotatus-8.jpg\n",
      "Mauia-subnotatus-9.jpg\n",
      "Araecerus-fasciculatus-10.jpg\n",
      "Araecerus-vieillardi-11.jpg\n",
      "Araeocorynus-cumingi-12.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "bulletin = 'b172'\n",
    "section = 'anthribidae-of-guam'\n",
    "\n",
    "# Extract bounding box images\n",
    "for i, r in df_bb_list.iterrows():\n",
    "    page_image_path = '{}/{}/{}'.format(bulletin, section, r.page_image)\n",
    "    im = cv2.imread(page_image_path)\n",
    "    roi = im[r.ytl:r.ybr, r.xtl:r.xbr]\n",
    "    roi_filename = r.bb_image\n",
    "    cv2.imwrite(roi_filename, roi)\n",
    "    print(roi_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Araecerus fasciculatus\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-6.jpg  176.57  1205.03  1146.23  1673.06\n",
      "\n",
      "Araecerus vieillardi\n",
      "                       image     xtl     ytl      xbr     ybr\n",
      "0  anthribidae-of-guam-7.jpg  276.38  246.42  1254.85  848.00\n",
      "\n",
      "Araeocorynus cumingi\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-7.jpg  290.87  1104.10  1247.61  1391.60\n",
      "\n",
      "Jordanthribus conspersus\n",
      "                       image     xtl     ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-2.jpg  186.83  947.16  1157.97  1692.34\n",
      "1  anthribidae-of-guam-3.jpg  271.55  256.08  1245.19  1009.87\n",
      "\n",
      "Jordanthribus planifascietus\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-1.jpg  282.43  1387.84  1243.02  1718.49\n",
      "1  anthribidae-of-guam-2.jpg  191.64   223.61  1162.78   932.73\n",
      "\n",
      "Mauia subnotatus\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-5.jpg  275.01  1459.78  1236.09  1721.23\n",
      "1  anthribidae-of-guam-6.jpg  181.37   218.57  1148.63   604.99\n",
      "\n",
      "Melanopsacus parvulus\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-4.jpg  190.75  1489.40  1167.69  1681.90\n",
      "1  anthribidae-of-guam-5.jpg  267.74   259.03  1236.09  1266.11\n",
      "\n",
      "Notioxenus fulgidus\n",
      "                       image     xtl      ytl      xbr      ybr\n",
      "0  anthribidae-of-guam-3.jpg  273.96  1251.47  1250.02  1727.42\n",
      "1  anthribidae-of-guam-4.jpg  188.35   221.31  1167.69  1474.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps\n",
    "\n",
    "sql = 'select name from df_bb_list group by name order by name'\n",
    "names = ps.sqldf(sql)\n",
    "for name in names.values:\n",
    "    name = name[0]\n",
    "    print(name)\n",
    "    sql = 'select image, xtl, ytl, xbr, ybr from df_bb_list where name=\"{}\" order by image'.format(name)\n",
    "    df = ps.sqldf(sql)\n",
    "    print(df)\n",
    "    \n",
    "    for i, r in df.iterrows():\n",
    "        bb = extract bb\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "## Step 1: Get PDF copies of Insects of Guam I and II (Bulletin 172 and 189)\n",
    "```\n",
    "mkdir B172\n",
    "cd B172\n",
    "wget --convert-links http://hbs.bishopmuseum.org/pubs-online/bpbm-bulletins.html\n",
    "grep -o 'hbs.bishopmuseum.org/pubs-online/pdf/b172p[1-9].*\\.pdf' bpbm-bulletins.html > B172-pdfs.txt\n",
    "wget -i B172-pdfs.txt\n",
    "cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.path.exists('caca'):\n",
    "    os.mkdir('caca')\n",
    "os.chdir('caca')\n",
    "print(os.getcwd())\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grep does not work\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "subprocess.call(['mkdir', 'B999'])\n",
    "os.chdir('B999')\n",
    "subprocess.call(['wget', '--convert-links', 'http://hbs.bishopmuseum.org/pubs-online/bpbm-bulletins.html'])\n",
    "\n",
    "outfile = open('titles-urls.htm', 'w')\n",
    "subprocess.call(['grep', 'hbs.bishopmuseum.org/pubs-online/pdf/b172p[1-9].*\\.pdf', 'bpbm-bulletins.html'],\n",
    "                shell=True, \n",
    "                stdout=outfile)\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract species sections from each PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.chdir('B172')\n",
    "filepath = 'titles-urls.htm'\n",
    "with open(filepath) as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "       #print(\"Line {}: {}\".format(cnt, line))\n",
    "       line = line.replace('&nbsp;', '').replace('[', '').replace(']', '')\n",
    "       line = line.replace(', by ', '|').replace('<img', '|').replace('href=\"', '|').replace('pdf\"', 'pdf|')\n",
    "       parts = line.split('|')\n",
    "       if (len(parts) == 5):\n",
    "            title = parts[0].strip()\n",
    "            authors = parts[1].strip()\n",
    "            url = parts[3].strip()\n",
    "            print('{}\\n{}\\n{}\\n\\n'.format(title, authors, url))\n",
    "            \n",
    "            directory = title.replace(' ','-')\n",
    "            subprocess.call(['mkdir', directory])\n",
    "            os.chdir(directory)\n",
    "            subprocess.call(['wget', url])\n",
    "            pdf = url.split('/')[-1]\n",
    "            #subprocess.call(['pdftk', pdf, 'burst'])\n",
    "            jpg = pdf.replace('pdf', 'jpg')\n",
    "            print(jpg)\n",
    "            subprocess.call(['convert', '-density', '200x200', pdf, jpg])\n",
    "            os.chdir('..')\n",
    "os.chdir('..')\n",
    "print('FINIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  \n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mturk_results(results_csv_filename):\n",
    "    \"\"\"\n",
    "    Returns a dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(results_csv_filename)\n",
    "    df = df[['Input.image_url', 'Answer.annotatedResult.boundingBoxes']]\n",
    "    df.columns =['image_url','bounding_boxes']\n",
    "\n",
    "    box_list = []\n",
    "    for i, r in df.iterrows():\n",
    "        image_fn = r['image_url'].split('/')[-1]\n",
    "        boxes = json.loads(r['bounding_boxes'])\n",
    "        for box in boxes:\n",
    "            box_dict = {\n",
    "                'image_fn': image_fn,\n",
    "                'box_type': box['label'],\n",
    "                'left': box['left'],\n",
    "                'top': box['top'],\n",
    "                'width': box['width'],\n",
    "                'height': box['height']\n",
    "            }\n",
    "            box_list.append(box_dict) \n",
    "    df_boxes = pd.DataFrame(box_list)\n",
    "    df_boxes.sort_values(['image_fn', 'top'], inplace=True)\n",
    "    \n",
    "    # Check that there is a max of one 'Species section orphan' per image_fn\n",
    "    # and that this section is nearest the top\n",
    "    \n",
    "    \n",
    "    return df_boxes\n",
    "\n",
    "#df = parse_mturk_results('Batch_235921_batch_results.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def check_orphans(df):\n",
    "    \"\"\"\n",
    "    Checks that each page contains 0 or 1 species section orphans.\n",
    "    Checks that, if an orphan exists, it precedes all species sections.\n",
    "    \"\"\"\n",
    "    for image_fn in df.image_fn.unique():\n",
    "        df_temp = df[df['image_fn']==image_fn].reset_index()\n",
    "        print image_fn, 'Number of sections:', len(df_temp)\n",
    "        orphan_count = len(df_temp[df_temp.box_type == 'Species section orphan'])\n",
    "        print image_fn, 'orphan_count:', orphan_count\n",
    "        if orphan_count > 1:\n",
    "            print image_fn, 'ERROR: More than 1 species section orphans on this page'\n",
    "            return\n",
    "        if orphan_count == 1:\n",
    "            orphan_index = df_temp[df_temp.box_type == 'Species section orphan'].index.item()\n",
    "            print image_fn, 'orphan_index:', orphan_index\n",
    "            if orphan_index > 0:\n",
    "                print image_fn, 'ERROR: Orphan is preceded by a species section on this page.'\n",
    "                return\n",
    "        print\n",
    "    return 'OK'\n",
    "\n",
    "#check_orphans(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_orphans():\n",
    "    \"\"\"\n",
    "    If the entry for a species section spans more that one page, there will be two images extracted,\n",
    "    saved with file names with include 'species section' and 'species section orphan'. \n",
    "    This function combines glues the 'species section orphan' images to the bottom of the preceeding\n",
    "    'species section' images.\n",
    "    \"\"\"\n",
    "    file_list = glob.glob('boxes/*.jpg')\n",
    "    file_list.sort()\n",
    "    number_of_images_combined = 0\n",
    "    for i, fn in enumerate(file_list):\n",
    "        if i>0:\n",
    "            fn_prev = file_list[i-1]\n",
    "            if 'orphan' in fn:\n",
    "                #print 'Combining {} and {}'.format(fn_prev, fn)\n",
    "                img1 = cv2.imread(fn_prev, 0)\n",
    "                img2 = cv2.imread(fn, 0)\n",
    "                new_img = combine_images_vertically(img1, img2) \n",
    "\n",
    "                # Overwrite the original 'species section' image file with the combined file.\n",
    "                cv2.imwrite(fn_prev, new_img)\n",
    "\n",
    "                # Delete the 'species section orphan' file.\n",
    "                os.remove(fn) \n",
    "                number_of_images_combined += 1\n",
    "    return number_of_images_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_mturk(df):\n",
    "    \"\"\"\n",
    "    REWRITE THIS\n",
    "    Box is a dict containing coordinates for a bounding box. Example:\n",
    "    \n",
    "        {'image_name': 'odonata-3.jpg',\n",
    "         'species_name': 'Tramea limbata',\n",
    "         'xbr': 1223,\n",
    "         'xtl': 250,\n",
    "         'ybr': 716,\n",
    "         'ytl': 348}\n",
    "         \n",
    "    This information to generate an image which is written to a file. Example:\n",
    "    \n",
    "        'boxes/Tramea limbata-odonata-3.jpg'    \n",
    "    \"\"\"\n",
    "    for i,r in df.iterrows():\n",
    "        #print r.image_fn\n",
    "        im = cv2.imread('odonata/'+r.image_fn)\n",
    "        #print r.top\n",
    "        #print r.height\n",
    "        #print r.left\n",
    "        #print r.width\n",
    "        roi = im[r.top: r.top+r.height-1, r.left:r.left+r.width-1]\n",
    "        roi_filename = 'boxes/{:03d}-{}-{}'.format(i, r.box_type, r.image_fn)\n",
    "        cv2.imwrite(roi_filename, roi)\n",
    "        #print roi_filename\n",
    "    return\n",
    "\n",
    "#extract_images_mturk(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def parse_cvat_xml(xml_file):\n",
    "    \"\"\"\n",
    "    Parses an XML file generated by CVAT.\n",
    "    Returns a list of dicts containing bounding box coordinates. \n",
    "    \"\"\"\n",
    "    box_list = []\n",
    "    tree = ET.parse(xml_file) \n",
    "    for image in tree.findall('image'):\n",
    "        for box in image.findall('box'):\n",
    "            box_list.append({\n",
    "                'image_name': image.get('name'),\n",
    "                'species_name':box.find('attribute').text,\n",
    "                'xtl': int(float(box.attrib.get('xtl'))),\n",
    "                'ytl': int(float(box.attrib.get('ytl'))),\n",
    "                'xbr': int(float(box.attrib.get('xbr'))),\n",
    "                'ybr': int(float(box.attrib.get('ybr')))           \n",
    "            })\n",
    "    return box_list        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def extract_image(box):\n",
    "    \"\"\"\n",
    "    Box is a dict containing coordinates for a bounding box. Example:\n",
    "    \n",
    "        {'image_name': 'odonata-3.jpg',\n",
    "         'species_name': 'Tramea limbata',\n",
    "         'xbr': 1223,\n",
    "         'xtl': 250,\n",
    "         'ybr': 716,\n",
    "         'ytl': 348}\n",
    "         \n",
    "    This information to generate an image which is written to a file. Example:\n",
    "    \n",
    "        'boxes/Tramea limbata-odonata-3.jpg'    \n",
    "    \"\"\"\n",
    "    im = cv2.imread(box['image_name'])\n",
    "    roi = im[box['ytl']:box['ybr'], box['xtl']:box['xbr']]\n",
    "    roi_filename = 'boxes/{}-{}'.format(box['species_name'], box['image_name'])\n",
    "    cv2.imwrite(roi_filename, roi)\n",
    "    return roi_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images_vertically(img1, img2):\n",
    "    \"\"\"\n",
    "    Glues 2 images together with img2 below img1.\n",
    "    Returns the new compound image.\n",
    "    \"\"\"\n",
    "    h1, w1 = img1.shape\n",
    "    h2, w2 = img2.shape\n",
    "\n",
    "    # Create an array big enough to hold img2 below img1.\n",
    "    img = np.zeros(((h1 + h2), max(w1, w2)), np.float32)\n",
    "\n",
    "    # Paste img1 at y=0, x=0\n",
    "    img[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "    # Paste img2 at y=h1, x=0\n",
    "    img[h1:h1+img2.shape[0],0:img2.shape[1]] = img2    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def glue_images():\n",
    "    \"\"\"\n",
    "    If the entry for a species spans more that one page, there will be two or more images extracted. This function\n",
    "    finds instances of multiple images for a species and pastes them together. \n",
    "    \n",
    "    This function should be repeated until the number_of_images_combined returned is zero:\n",
    "    \n",
    "        while True:\n",
    "            if glue_boxes() == 0:\n",
    "                break\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = glob.glob('boxes/*.jpg')\n",
    "    file_list.sort()\n",
    "\n",
    "    number_of_images_combined = 0\n",
    "    for i, fn in enumerate(file_list):\n",
    "        if i>0:\n",
    "            fn_prev = file_list[i-1]\n",
    "            if fn.split('-')[0] == fn_prev.split('-')[0]:\n",
    "                print 'Combining {} and {}'.format(fn_prev, fn)\n",
    "                number_of_images_combined += 1\n",
    "\n",
    "                img1 = cv2.imread(fn_prev, 0)\n",
    "                img2 = cv2.imread(fn, 0)\n",
    "                h1, w1 = img1.shape\n",
    "                h2, w2 = img2.shape\n",
    "\n",
    "                # Create an array big enough to hold img2 below img1.\n",
    "                new = np.zeros(((h1 + h2), max(w1, w2)), np.float32)\n",
    "\n",
    "                # Paste img1 at y=0, x=0\n",
    "                new[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "                # Paste img2 at y=h1, x=0\n",
    "                new[h1:h1+img2.shape[0],0:img2.shape[1]] = img2\n",
    "\n",
    "                # Overwrite fn_prev\n",
    "                cv2.imwrite(fn_prev, new)\n",
    "\n",
    "                # Delete fn\n",
    "                os.remove(fn) \n",
    "    return number_of_images_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PDF\n",
    "```\n",
    "wget http://hbs.bishopmuseum.org/pubs-online/pdf/b172p3-6.pdf -O odonata.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF into a set of JPGs\n",
    "\n",
    "```\n",
    "convert -density 200x200 odonata.pdf odonata.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place bounding boxes around ROIs using MTurk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<!-- You must include this JavaScript file -->\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<!-- For the full list of available Crowd HTML Elements and their input/output documentation,\n",
    "      please refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html -->\n",
    "\n",
    "<!-- You must include crowd-form so that your task submits answers to MTurk -->\n",
    "<crowd-form answer-format=\"flatten-objects\">\n",
    "\n",
    "    <!-- The crowd-bounding-box element will create a tool for the Worker to draw \n",
    "           labeled boxes around the specified objects in your image.\n",
    "\n",
    "          Your image file URLs will be substituted for the \"image_url\" variable below \n",
    "          when you publish a batch with a CSV input file containing multiple image file URLs.\n",
    "          To preview the element with an example image, try setting the src attribute to\n",
    "          \"https://s3.amazonaws.com/cv-demo-images/two-birds.jpg\" -->\n",
    "    <crowd-bounding-box \n",
    "        src=\"${image_url}\"\n",
    "        labels=\"['Species section', 'Species section orphan']\"\n",
    "        header=\"Draw bounding boxes around the requested items\"\n",
    "        name=\"annotatedResult\">\n",
    "\n",
    "        <!-- Use the short-instructions section for quick instructions that the Worker\n",
    "              will see while working on the task. Including some basic examples of \n",
    "              good and bad answers here can help get good results. You can include \n",
    "              any HTML here. -->\n",
    "        <short-instructions>Draw boxes around the requested target of interest.</short-instructions>\n",
    "\n",
    "        <!-- Use the full-instructions section for more detailed instructions that the \n",
    "              Worker can open while working on the task. Including more detailed \n",
    "              instructions and additional examples of good and bad answers here can\n",
    "              help get good results. You can include any HTML here. -->\n",
    "        <full-instructions header=\"Bounding Box Instructions\">\n",
    "            <p>Use the bounding box tool to draw boxes around the requested target of interest:</p>\n",
    "            <ol>\n",
    "              \t<li>Draw a rectangle using your mouse over each instance of the target.</li>\n",
    "                <li>Make sure the box does not cut into the target, leave a 2 - 3 pixel margin</li>\n",
    "               \t<li>When targets are overlapping, draw a box around each object, include all \n",
    "                      contiguous parts of the target in the box. Do not include parts that are completely \n",
    "                      overlapped by another object.</li>\n",
    "               \t<li>Do not include parts of the target that cannot be seen, even though you think you \n",
    "                      can interpolate the whole shape of the target.</li>\n",
    "               \t<li>Avoid shadows, they're not considered as a part of the target.</li>\n",
    "               \t<li>If the target goes off the screen, label up to the edge of the image.</li>\n",
    "            </ol>\n",
    "        </full-instructions>\n",
    "\n",
    "    </crowd-bounding-box>\n",
    "</crowd-form>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract bounding boxes as a set of JPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mturk_results_csv = 'Batch_235921_batch_results.csv'\n",
    "\n",
    "df = parse_mturk_results(mturk_results_csv)\n",
    "extract_images_mturk(df)\n",
    "attach_orphans();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update GitHub repository\n",
    "```bash\n",
    "git add .\n",
    "git commit -m 'add species section images'\n",
    "git push\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from species section images using MTurk\n",
    "\n",
    "```html\n",
    "<head>\n",
    "  <title>My Design with Bootstrap</title>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
    "  <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\">\n",
    "  <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js\"></script>\n",
    "  <script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js\"></script>\n",
    "  <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\"></script>\n",
    "  <!-- You must include this JavaScript file -->\n",
    "  <script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "   <div class=\"container\">\n",
    "        <div class=\"row\">\n",
    "            <div class=\"col-sm-6\">\n",
    "                <p><img src=\"${image_url}\" style=\"max-width: 100%\" /></p>\n",
    "                <p class=\"small\">${text}</p>\n",
    "            </div>\n",
    "            <div class=\"col-sm-6\">\n",
    "                <crowd-form answer-format=\"flatten-objects\">\n",
    "                    <p><strong>Instructions: </strong></p>\n",
    "                    <div><crowd-input label=\"Scientific name\" name=\"scientific_name\" required></div>\n",
    "                    <div><crowd-text-area label=\"Collection records\" name=\"collection_records\"></div>\n",
    "                    <div><crowd-checkbox name=\"first_record\"> First record for Guam</div>\n",
    "                </crowd-form>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aubreymoore/insects-of-guam/raw/master/boxes/000-Species%20section-odonata-0.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv file containing image_urls and text (html)\n",
    "# This file will be used by MTurk\n",
    "\n",
    "prefix = 'https://github.com/aubreymoore/insects-of-guam-datamining/raw/master/'\n",
    "filelist = glob.glob('boxes/*.jpg')\n",
    "filelist.sort()    \n",
    "with open('mturk_data.csv', 'w+') as f:\n",
    "    f.write('image_url,text\\n')\n",
    "    for fn in filelist:\n",
    "        image_url = prefix + fn\n",
    "        print(image_url)\n",
    "        \n",
    "        # Perform optical character recognition on the image\n",
    "        #text = pytesseract.image_to_string(Image.open(fn)).encode('utf-8')\n",
    "        text = pytesseract.image_to_string(Image.open(fn))\n",
    "        \n",
    "        # Remove double quotes and replace line feeds with <br>\n",
    "        text = text.replace('\\n\\n', '\\n')\n",
    "        text = text.replace('\"', '').replace('\\n', '<br>')\n",
    "        \n",
    "        # Separate collection records into individual lines of text\n",
    "        text = text.replace(';', '<br>')\n",
    "        \n",
    "        f.write('{},\"{}\"\\n'.format(image_url, text))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if not os.path.exists('ocr'):\n",
    "    os.makedirs('ocr')\n",
    "filelist = glob.glob('boxes/*.jpg')\n",
    "filelist.sort()\n",
    "for fn in filelist:\n",
    "    text = pytesseract.image_to_string(Image.open(fn))\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Separate collection records into individual lines of text\n",
    "    text = text.replace('; ', '\\n')\n",
    "\n",
    "    fn = fn.replace('boxes', 'ocr')\n",
    "    fn = fn.replace('.jpg', '.txt')\n",
    "    with open(fn, 'w+') as f:\n",
    "        f.write(text.encode('utf-8')) \n",
    "    print fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob.glob('boxes/*.jpg')\n",
    "filelist\n",
    "\n",
    "fn = filelist[5]\n",
    "text = pytesseract.image_to_string(Image.open(fn))\n",
    "text = text.replace('\\n\\n', '\\n')\n",
    "\n",
    "# Separate collection records into individual lines of text\n",
    "text = text.replace('; ', '\\n')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Agrionaptera insignis insignis (Rambur)\n",
    "\n",
    "Orote Peninsula | April 9 | Bryan\n",
    "Yigo | April 13 | Bryan\n",
    "Mt. Alifan | April 20 | Bryan\n",
    "Mt. Alifan | April 21 | Bryan\n",
    "Mt. Alifan | April 30 | Bryan\n",
    "Agana | May 4 | Bryan, Swezey, Usinger\n",
    "Agana | May 25 | Bryan, Swezey, Usinger\n",
    "Inarajan | May 7 | Swezey\n",
    "Umatac | May 14 | Usinger\n",
    "Dededo | Aug. 11 | Swezey\n",
    "\n",
    "First Island Record"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
